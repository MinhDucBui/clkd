defaults:
  - ../tokenizer@tokenizer: autotokenizer.yaml

model:
  _target_: src.models.model.get_xlmr
  pretrained_model_name_or_path: "xlm-roberta-base"
  use_pretrained_weights: true
  cfg:
    # Architecture (Default architecture):
    attention_probs_dropout_prob: 0.1
    bos_token_id: 0
    classifier_dropout: null
    eos_token_id: 2
    gradient_checkpointing: false
    hidden_act: "gelu"
    hidden_dropout_prob: 0.1
    hidden_size: 768
    initializer_range: 0.02
    intermediate_size: 3072
    layer_norm_eps: 1e-12
    max_position_embeddings: 512
    model_type: "xlm-roberta"
    num_attention_heads: 12
    num_hidden_layers: 12
    pad_token_id: 1
    position_embedding_type: "absolute"
    transformers_version: "4.10.0"
    type_vocab_size: 2
    use_cache: true
    vocab_size: 30522

# Define Tokenizer for teacher/student
tokenizer:
  pretrained_model_name_or_path: ${..model.pretrained_model_name_or_path}

