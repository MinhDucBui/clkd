_recursive_: False
_target_: src.models.model.get_bert

pretrained_model_name_or_path: "bert-base-cased"
use_pretrained_weights: true
cfg:
  # Architecture (Default BERT architecture):
  vocab_size: 30522 # is being automatically updated based on the tokenizer
  hidden_size: 768
  num_hidden_layers: 12
  num_attention_heads: 12
  intermediate_size: 3072
  hidden_act: 'gelu'
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1
  max_position_embeddings: 512
  type_vocab_siz: 2
  initializer_range: 0.02
  layer_norm_eps: 1e-12
  pad_token_id: 0
  position_embedding_type: 'absolute'
  use_cache: True
  classifier_dropout: None
  output_hidden_states: True
  output_attentions: True


